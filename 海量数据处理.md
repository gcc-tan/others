面试或者实际处理的过程中总会遇到海量数据的问题，海量数据是指时间（短时间内无法解决）或者空间上（无法装入内存）的量大。

针对时间问题，可以采用巧妙的算法搭配合适的数据结构进行加速。对于空间问题，可以采用分而制之的方法缩小问题规模。下面介绍一些典型的问题，和针对这类问题的处理方法

###分治
分治策略主要的两个过程就是划分和合并。划分需要将问题划分成若干不相关的子问题，对子问题求解完成之后需要合并成原问题的结果。

划分的过程中常用的技巧就是hash映射。

例子1：**海量日志数据，提取出某日访问百度次数最多的那个IP**

分析：正确的做法是首先从日志中提取IP地址，存储到某个文件中。这个文件肯定加载不进内存，需要对文件进行切割。
1. 于是采用hash映射的方式。最简单的hash就是对IP地址%1000，得到的hash值作为文件名字,将IP地址存入该文件中。这样就得到了1000个小文件。此处采用hash还保证了相同的IP地址会映射到同样一个文件中，最后统计IP地址出现次数时，不需要考虑其他文件中出现的情况。
2. 对每个小文件中采用hash_map(ip, count)来计算词频，同时找出出现次数最大的IP
3. 最后找出1000个里面出现次数最大的就行

>如果需要找出访问次数前10的IP地址，那么需要在每个小文件中找出IP访问前10。而在每个小文件中找出前10可以利用最小堆，堆中的元素表示每个IP地址的访问次数。堆的大小是10。遍历hash_map(ip, count)中的条目，更新堆中元素即可

例子2：**寻找5亿个int数的中位数**

分析：这里假设数据不能加载进入内存。乍一看不知道怎么划分。其实仔细想想还是很简单的。找到一个合适的尺度例如$2^{30}$，那么int类型的数据范围$2^{32}$，也就是这个尺度范围将int类型的数划分成4个区域，统计每个区域的数字数量，然后可以计算中位数落入哪个区域，并且可以得到这个区域的第几大的数。问题就转换成数据规模更小的top K问题。top K问题解决方法就是使用堆。




###Bitmap
见有关bitmap的介绍

###Bloom Filter
见关于bloom filter的介绍

内容来自：

[海量数据处理](https://www.kancloud.cn/kancloud/the-art-of-programming/41608)